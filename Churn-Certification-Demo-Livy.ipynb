{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Customer Churn Demo: Remote Spark Livy\n",
    "\n",
    "In this notebook you will build a predictive model with Spark machine learning(SparkML) and deploy the model in Machine Learning (ML) in REST endpoint using HDP and IBM DSX. This notebook walks you through following steps:\n",
    "\n",
    "- Fetching data from HDFS\n",
    "- Feature engineering\n",
    "- Data Visualization\n",
    "- Build a binary classifier model with SparkML API\n",
    "- Save the model in the ML repository\n",
    "- Deploy model online(via UI)\n",
    "- Test the model (via UI)\n",
    "- Test the model (via REST API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "\n",
    "The analytics use case implemented in this notebook is telco churn. While it's a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice for implementing predictive analytics. \n",
    "![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n",
    "\n",
    "The analytics process starts with defining the business problem and identifying the data that can be used to solve the problem. For Telco churn, we use demographic and historical transaction data. We also know which customers have churned, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). \n",
    "\n",
    "Once the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a \"class\" to each customer record (for our use case \"churn\" or \"no churn\"). Classification models use historical data to come up with the logic to predict \"class\", this process is called model training. After the model is created, it's usually evaluated using another data set. \n",
    "\n",
    "Finally, if the model's accuracy meets the expectations, it can be deployed for scoring. Scoring is the process of applying the model to a new set of data. For example, when we receive new transactional data, we can score the customer for the risk of churn.  \n",
    "\n",
    "We also developed a sample Python Flask application to illustrate deployment: http://predictcustomerchurn.mybluemix.net/. This application implements the REST client call to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I: Set up remote Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark magics\n",
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://172.26.192.223:8999\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1509554242062_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://edwdemo0.field.hortonworks.com:8088/proxy/application_1509554242062_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://edwdemo3.field.hortonworks.com:8042/node/containerlogs/container_e10_1509554242062_0005_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "# http://172.26.192.223:8999\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# II. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data validation over webHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  272k  100  272k    0     0  2791k      0 --:--:-- --:--:-- --:--:-- 2791k\n",
      "3821,\"F\",\"S\",0.000000,78851.300000,\"N\",48.373333,0.370000,0.000000,28.660000,0.000000,\"CC\",\"FreeLocal\",\"Standard\",29.040000,4.000000\n",
      "3822,\"F\",\"S\",1.000000,17540.700000,\"Y\",62.786667,22.170000,0.570000,13.450000,0.000000,\"Auto\",\"Budget\",\"Standard\",36.200000,1.000000\n",
      "3823,\"F\",\"M\",0.000000,83891.900000,\"Y\",61.020000,28.920000,0.000000,45.470000,0.000000,\"CH\",\"Budget\",\"Standard\",74.400000,4.000000\n",
      "3824,\"F\",\"M\",2.000000,28220.800000,\"N\",38.766667,26.490000,0.000000,12.460000,0.000000,\"CC\",\"FreeLocal\",\"Standard\",38.950000,4.000000\n",
      "3825,\"F\",\"S\",0.000000,28589.100000,\"N\",15.600000,13.190000,0.000000,87.090000,0.000000,\"CC\",\"FreeLocal\",\"Standard\",100.280000,3.000000\n"
     ]
    }
   ],
   "source": [
    "# Chustomer data\n",
    "!curl -i -L \"http://edwdemo0.field.hortonworks.com:50070/webhdfs/v1/user/dsx_datasets/customer.csv?op=OPEN\" | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 20079  100 20079    0     0   263k      0 --:--:-- --:--:-- --:--:--  263k\n",
      "3821,\"T\"\n",
      "3822,\"T\"\n",
      "3823,\"T\"\n",
      "3824,\"T\"\n",
      "3825,\"T\"\n"
     ]
    }
   ],
   "source": [
    "# Churn data\n",
    "!curl -i -L \"http://edwdemo0.field.hortonworks.com:50070/webhdfs/v1/user/dsx_datasets/churn.csv?op=OPEN\" | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Load\n",
    "**Note:** Once you have run the below row you can check up your Yarn UI to verify a job has been triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "| ID|Gender|Status|Children|Est Income|Car Owner|      Age|LongDistance|International| Local|Dropped|Paymethod|LocalBilltype|LongDistanceBilltype| Usage|RatePlan|\n",
      "+---+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "|  1|     F|     S|     1.0|   38000.0|        N|24.393333|       23.56|          0.0|206.08|    0.0|       CC|       Budget|      Intnl_discount|229.64|     3.0|\n",
      "|  6|     M|     M|     2.0|   29616.0|        N|49.426667|       29.78|          0.0|  45.5|    0.0|       CH|    FreeLocal|            Standard| 75.29|     2.0|\n",
      "|  8|     M|     M|     0.0|   19732.8|        N|50.673333|       24.81|          0.0| 22.44|    0.0|       CC|    FreeLocal|            Standard| 47.25|     3.0|\n",
      "| 11|     M|     S|     2.0|     96.33|        N|56.473333|       26.13|          0.0| 32.88|    1.0|       CC|       Budget|            Standard| 59.01|     1.0|\n",
      "| 14|     F|     M|     2.0|   52004.8|        N|    25.14|        5.03|          0.0| 23.11|    0.0|       CH|       Budget|      Intnl_discount| 28.14|     1.0|\n",
      "+---+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-----+\n",
      "| ID|CHURN|\n",
      "+---+-----+\n",
      "|  1|    T|\n",
      "|  6|    F|\n",
      "|  8|    F|\n",
      "| 11|    F|\n",
      "| 14|    F|\n",
      "+---+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Customer Information\n",
    "# Add asset from file system\n",
    "customer = SQLContext(sc).read.csv('hdfs://edwdemo0.field.hortonworks.com:8020/user/dsx_datasets/customer.csv', header=True, inferSchema=True)\n",
    "#customer = sqlContext(sc).load(source=\"hdfs://edwdemo0.field.hortonworks.com:8020/user/dsx_datasets\", header=\"true\", path = \"customer.csv\")\n",
    "#customer = SQLContext(sc).read.format(\"csv\").options(header=\"true\").options(inferSchema=\"true\").load(\"hdfs://edwdemo0.field.hortonworks.com:8020/user/dsx_datasets/customer.csv\")\n",
    "customer.show(5)\n",
    "\n",
    "\n",
    "#Churn information    \n",
    "# Add asset from file system\n",
    "customer_churn = SQLContext(sc).read.csv('hdfs://edwdemo0.field.hortonworks.com:8020/user/dsx_datasets/churn.csv', header=True, inferSchema=True)\n",
    "#customer_churn = sqlContext(sc).load(source=\"hdfs://edwdemo0.field.hortonworks.com:8020/user/dsx_datasets\", header=\"true\", path = \"churn.csv\")\n",
    "#customer_churn = SQLContext(sc).read.format(\"csv\").options(header=\"true\").options(inferSchema=\"true\").load(\"hdfs://edwdemo0.field.hortonworks.com:8020/user/dsx_datasets/churn.csv\")\n",
    "customer_churn.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 with DSX Spark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
