{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trucking Demo with IBM DSX\n",
    "\n",
    "Task involve Data Ingestion, Data Formatting, Exploratory Analysis and Model Building. Data Science involves a typical sequence of tasks: \n",
    "- aquiring data,\n",
    "- cleaning it, \n",
    "- analyzing it for relationships, and\n",
    "- building a model. \n",
    "\n",
    "**Note:** Here we are submitting spark jobs locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Inpect the data on HDFS in remote HDP cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 63570  100 63570    0     0   743k      0 --:--:-- --:--:-- --:--:--  743k\n",
      "Overspeed,\"Y\",\"hours\",45,2773,-90.07,35.68,0,1,1\n",
      "Lane Departure,\"Y\",\"hours\",45,2773,-90.04,35.19,1,1,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-90.68,35.12,1,0,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-91.14,34.96,0,0,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-91.93,34.81,0,0,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-92.31,34.78,0,1,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-92.09,34.8,0,0,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-91.93,34.81,0,0,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-90.68,35.12,0,0,0\n",
      "Normal,\"Y\",\"hours\",45,2773,-91.74,34.89,0,0,0\n"
     ]
    }
   ],
   "source": [
    "# events data\n",
    "!curl -i -L \"http://172.26.232.240:50070/webhdfs/v1/tmp/enrichedEvents?op=OPEN\" | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 33084  100 33084    0     0   489k      0 --:--:-- --:--:-- --:--:--  489k\n",
      "1, 0 0 0.45 0.2773 1 1 1\n",
      "1, 0 0 0.45 0.2773 1 0 0\n",
      "0, 0 0 0.45 0.2773 1 0 0\n",
      "0, 0 0 0.45 0.2773 0 0 0\n",
      "0, 0 0 0.45 0.2773 0 1 0\n",
      "0, 0 0 0.45 0.2773 0 0 0\n",
      "0, 0 0 0.45 0.2773 0 0 1\n",
      "0, 0 0 0.45 0.2773 0 0 1\n",
      "0, 0 0 0.45 0.2773 0 1 0\n",
      "0, 0 0 0.45 0.2773 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# training Data\n",
    "!curl -i -L \"http://172.26.232.240:50070/webhdfs/v1/tmp/trainingData?op=OPEN\" | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second: Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import library and set a spark session:\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "#sc = SparkContext()\n",
    "sparkSession = SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Events Data : from HDFS\n",
    "#eventsFile = sc.textFile(\"hdfs://172.26.232.240:8020/tmp/enrichedEvents\") # Using this option will load data as pipelined RDD which will need transformation\n",
    "eventsFile = sparkSession.read.csv(\"hdfs://172.26.232.240:8020/tmp/enrichedEvents\", header = \"false\", inferSchema = \"false\")  # this will load it as Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eventsFile.count())\n",
    "type(eventsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-----+---+----+------+-----+---+---+---+\n",
      "|           _c0|_c1|  _c2|_c3| _c4|   _c5|  _c6|_c7|_c8|_c9|\n",
      "+--------------+---+-----+---+----+------+-----+---+---+---+\n",
      "|        Normal|  N|miles| 70|3300|-95.01|36.73|  0|  1|  1|\n",
      "|Lane Departure|  N|miles| 70|3300|-91.99|37.94|  0|  0|  0|\n",
      "|Lane Departure|  N|miles| 70|3300|-92.08|37.81|  0|  1|  1|\n",
      "|Lane Departure|  N|miles| 70|3300| -95.5|36.37|  1|  1|  1|\n",
      "|Lane Departure|  N|miles| 70|3300|-94.23|37.09|  1|  1|  1|\n",
      "+--------------+---+-----+---+----+------+-----+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the data\n",
    "eventsFile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# infer schema\n",
    "eventsFile.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning Names to fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventTyp: string (nullable = true)\n",
      " |-- isCertified: string (nullable = true)\n",
      " |-- paymentScheme: string (nullable = true)\n",
      " |-- hoursDriven: string (nullable = true)\n",
      " |-- milesDriven: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- isFoggy: string (nullable = true)\n",
      " |-- isRainy: string (nullable = true)\n",
      " |-- isWindy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_col_names = eventsFile.columns\n",
    "new_col_names =['eventTyp', 'isCertified', 'paymentScheme', 'hoursDriven', 'milesDriven', 'latitude', 'longitude', 'isFoggy', 'isRainy', 'isWindy']\n",
    "# Renaming the columns\n",
    "eventsdata = reduce(lambda eventsFile, idx: eventsFile.withColumnRenamed(old_col_names[idx], new_col_names[idx]), range(len(old_col_names)), eventsFile)\n",
    "eventsdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type conversion for Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=eventsdata.withColumn(\"latitude\", eventsdata[\"latitude\"].cast(\"float\")).withColumn(\"longitude\", eventsdata[\"longitude\"].cast(\"float\")).withColumn(\"hoursDriven\", eventsdata[\"hoursDriven\"].cast(\"int\")).withColumn(\"isFoggy\", eventsdata[\"isFoggy\"].cast(\"int\")).withColumn(\"isRainy\", eventsdata[\"isRainy\"].cast(\"int\")).withColumn(\"isWindy\", eventsdata[\"isWindy\"].cast(\"int\")).withColumn(\"milesDriven\", eventsdata[\"milesDriven\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- eventTyp: string (nullable = true)\n",
      " |-- isCertified: string (nullable = true)\n",
      " |-- paymentScheme: string (nullable = true)\n",
      " |-- hoursDriven: integer (nullable = true)\n",
      " |-- milesDriven: integer (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- isFoggy: integer (nullable = true)\n",
      " |-- isRainy: integer (nullable = true)\n",
      " |-- isWindy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "# view final schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register table for Enriched Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.registerTempTable(\"enrichedEvents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "|      eventTyp|isCertified|paymentScheme|hoursDriven|milesDriven|latitude|longitude|isFoggy|isRainy|isWindy|\n",
      "+--------------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "|        Normal|          N|        miles|         70|       3300|  -95.01|    36.73|      0|      1|      1|\n",
      "|Lane Departure|          N|        miles|         70|       3300|  -91.99|    37.94|      0|      0|      0|\n",
      "|Lane Departure|          N|        miles|         70|       3300|  -92.08|    37.81|      0|      1|      1|\n",
      "|Lane Departure|          N|        miles|         70|       3300|   -95.5|    36.37|      1|      1|      1|\n",
      "|Lane Departure|          N|        miles|         70|       3300|  -94.23|    37.09|      1|      1|      1|\n",
      "+--------------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shape', 1359, ['eventTyp', 'isCertified', 'paymentScheme', 'hoursDriven', 'milesDriven', 'latitude', 'longitude', 'isFoggy', 'isRainy', 'isWindy'])\n"
     ]
    }
   ],
   "source": [
    "print(\"shape\",data.count(),data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Data : from HDFS\n",
    "trainingFile = sparkSession.read.csv(\"hdfs://172.26.232.240:8020/tmp/trainingData\", header = \"false\", inferSchema = \"false\")  # this will load it as Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'pyspark.sql.dataframe.DataFrame'>, 1359)\n"
     ]
    }
   ],
   "source": [
    "print(type(trainingFile), trainingFile.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|_c0|                _c1|\n",
      "+---+-------------------+\n",
      "|  0| 0 0 0.7 0.33 0 0 1|\n",
      "|  1| 0 0 0.7 0.33 1 1 0|\n",
      "|  1| 0 0 0.7 0.33 1 1 1|\n",
      "|  1| 0 0 0.7 0.33 1 0 0|\n",
      "|  1| 0 0 0.7 0.33 1 0 1|\n",
      "+---+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# columna= [\"violation\", 'isCertified', 'paymentScheme', 'hoursDriven', 'milesDriven', 'isFoggy', 'isRainy', 'isWindy'])\n",
    "trainingFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third: Exploratory analysis    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import brunel\n",
    "#df = eventsRDD1.toPandas()\n",
    "# Leaving this for later time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth: building regression model for violation prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique truck events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Normal',\n",
       " u'Lane Departure',\n",
       " u'Overspeed',\n",
       " u'Unsafe following distance',\n",
       " u'Unsafe tail distance']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truck_events= list(data.toPandas()['eventTyp'].unique())\n",
    "truck_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming eventType column\n",
    "\n",
    "*eventType -> ifViolated*\n",
    "\n",
    "- *N - 'Normal'*\n",
    "\n",
    "- *Y - 'Lane Departure', 'Overspeed','Unsafe following distance', 'Unsafe tail distance']*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform column eventType\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "name = 'eventTyp'\n",
    "udf = UserDefinedFunction(lambda x: 'N' if x==\"Normal\" else 'Y', StringType())\n",
    "data_transformed=data.select(*[udf(column).alias(name) if column == name else column for column in data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventTyp: string (nullable = true)\n",
      " |-- isCertified: string (nullable = true)\n",
      " |-- paymentScheme: string (nullable = true)\n",
      " |-- hoursDriven: integer (nullable = true)\n",
      " |-- milesDriven: integer (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- isFoggy: integer (nullable = true)\n",
      " |-- isRainy: integer (nullable = true)\n",
      " |-- isWindy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'N', u'Y']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations = list(data_transformed.toPandas()['eventTyp'].unique())\n",
    "violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "|eventTyp|isCertified|paymentScheme|hoursDriven|milesDriven|latitude|longitude|isFoggy|isRainy|isWindy|\n",
      "+--------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "|       N|          N|        miles|         70|       3300|  -95.01|    36.73|      0|      1|      1|\n",
      "|       Y|          N|        miles|         70|       3300|  -91.99|    37.94|      0|      0|      0|\n",
      "|       Y|          N|        miles|         70|       3300|  -92.08|    37.81|      0|      1|      1|\n",
      "|       Y|          N|        miles|         70|       3300|   -95.5|    36.37|      1|      1|      1|\n",
      "|       Y|          N|        miles|         70|       3300|  -94.23|    37.09|      1|      1|      1|\n",
      "+--------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building model > RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Using random forest classification rather than plain logistic regression.\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "# StringIndexer encodes a string column of labels to a column of label indices\n",
    "SI1 = StringIndexer(inputCol='isCertified',outputCol='isCertifiedEncoded')\n",
    "SI2 = StringIndexer(inputCol='paymentScheme',outputCol='paymentSchemeEncoded')\n",
    "\n",
    "#encode the Label column\n",
    "labelIndexer = StringIndexer(inputCol='eventTyp', outputCol='label').fit(data_transformed)\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"isCertifiedEncoded\", \"paymentSchemeEncoded\", \"hoursDriven\", \"milesDriven\", \"latitude\", \\\n",
    "                                       \"longitude\", \"isFoggy\", \"isRainy\", \"isWindy\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[SI1,SI2,labelIndexer, assembler, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[eventTyp: string, isCertified: string, paymentScheme: string, hoursDriven: int, milesDriven: int, latitude: float, longitude: float, isFoggy: int, isRainy: int, isWindy: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = data_transformed.randomSplit([0.8,0.2], seed=6)\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[eventTyp: string, isCertified: string, paymentScheme: string, hoursDriven: int, milesDriven: int, latitude: float, longitude: float, isFoggy: int, isRainy: int, isWindy: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventTyp: string (nullable = true)\n",
      " |-- isCertified: string (nullable = true)\n",
      " |-- paymentScheme: string (nullable = true)\n",
      " |-- hoursDriven: integer (nullable = true)\n",
      " |-- milesDriven: integer (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- isFoggy: integer (nullable = true)\n",
      " |-- isRainy: integer (nullable = true)\n",
      " |-- isWindy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "|eventTyp|isCertified|paymentScheme|hoursDriven|milesDriven|latitude|longitude|isFoggy|isRainy|isWindy|\n",
      "+--------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "|       N|          N|        miles|          0|          0|  -94.59|     39.1|      0|      0|      0|\n",
      "|       N|          N|        miles|          0|          0|  -94.58|    37.03|      0|      0|      0|\n",
      "|       N|          N|        miles|          0|          0|  -94.46|    37.16|      1|      1|      1|\n",
      "|       N|          N|        miles|          0|          0|  -94.38|    38.99|      0|      0|      1|\n",
      "|       N|          N|        miles|          0|          0|  -91.94|    41.71|      0|      0|      0|\n",
      "+--------+-----------+-------------+-----------+-----------+--------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventTyp</th>\n",
       "      <th>label</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.71386659273, 0.28613340727]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.715791195904, 0.284208804096]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.37125, 0.62875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.565553100666, 0.434446899334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.829561037174, 0.170438962826]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eventTyp  label predictedLabel  prediction                       probability\n",
       "0        N    0.0              N         0.0    [0.71386659273, 0.28613340727]\n",
       "1        N    0.0              N         0.0  [0.715791195904, 0.284208804096]\n",
       "2        N    0.0              Y         1.0                [0.37125, 0.62875]\n",
       "3        N    0.0              N         0.0  [0.565553100666, 0.434446899334]\n",
       "4        N    0.0              N         0.0  [0.829561037174, 0.170438962826]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=results.select(results[\"eventTyp\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\n",
    "results.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventTyp: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- predictedLabel: string (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision model = 0.92.\n"
     ]
    }
   ],
   "source": [
    "print ('Precision model = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve = 0.68.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth: Save Model in ML repositor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from repository.mlrepositoryclient import MLRepositoryClient\n",
    "from repository.mlrepositoryartifact import MLRepositoryArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443'\n",
    "ml_repository_client = MLRepositoryClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model artifact (abstraction layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Predict_Violations v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pipeline and model artifacts to in Machine Learning repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model = ml_repository_client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved model properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelType: sparkml-model-2.0\n",
      "creationTime: 2017-10-20 21:43:35.773000+00:00\n",
      "modelVersionHref: https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443/v2/artifacts/models/a3cce819-b358-4ab7-9254-00188765d1cd/versions/e33313ff-7e3e-4f22-bd49-787c4ee94e69\n",
      "label: eventTyp\n"
     ]
    }
   ],
   "source": [
    "print \"modelType: \" + saved_model.meta.prop(\"modelType\")\n",
    "print \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\n",
    "print \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\n",
    "print \"label: \" + saved_model.meta.prop(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth: Deploy and Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 with DSX Spark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
